{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalCDCCleaner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChsAzfpaexMq",
        "outputId": "47709bcc-a900-48de-b416-5cd62181fad4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPSOZtW815yy"
      },
      "source": [
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from nltk.stem.porter import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le9Iau0o1-hU"
      },
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vku3SLSU1_b7"
      },
      "source": [
        "def clean_csv_state(current_csv_state):\n",
        "\n",
        "  # add it in\n",
        "  current_csv_state['tidy_state'] = np.vectorize(remove_pattern)(current_csv_state['state'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_state['tidy_state'] = current_csv_state['tidy_state'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_state = current_csv_state['tidy_state'].apply(lambda x: x.split())\n",
        "  tokenized_state.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_state = tokenized_state.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_state.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_state)):\n",
        "      tokenized_state[i] = ' '.join(tokenized_state[i])\n",
        "\n",
        "  current_csv_state['tidy_state'] = tokenized_state\n",
        "  current_csv_state['tidy_state'] = current_csv_state['tidy_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_7ZxgZg2A4S"
      },
      "source": [
        "def clean_csv_cityorcounty(current_csv_coc):\n",
        " \n",
        "  # add it in\n",
        "  current_csv_coc['tidy_coc'] = np.vectorize(remove_pattern)(current_csv_coc['city_or_county'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_coc['tidy_coc'] = current_csv_coc['tidy_coc'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_coc = current_csv_coc['tidy_coc'].apply(lambda x: x.split())\n",
        "  tokenized_coc.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_coc = tokenized_coc.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_coc.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_coc)):\n",
        "      tokenized_coc[i] = ' '.join(tokenized_coc[i])\n",
        "\n",
        "  current_csv_coc['tidy_coc'] = tokenized_coc\n",
        "  current_csv_coc['tidy_coc'] = current_csv_coc['tidy_coc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32rViGJi2CSz"
      },
      "source": [
        "def clean_csv_address(current_csv_address):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_address['address']=current_csv_address['address'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_address['tidy_address'] = np.vectorize(remove_pattern)(current_csv_address['address'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_address['tidy_address'] = current_csv_address['tidy_address'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_address = current_csv_address['tidy_address'].apply(lambda x: x.split())\n",
        "  tokenized_address.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_address = tokenized_address.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_address.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_address)):\n",
        "      tokenized_address[i] = ' '.join(tokenized_address[i])\n",
        "\n",
        "  current_csv_address['tidy_address'] = tokenized_address\n",
        "  current_csv_address['tidy_address'] = current_csv_address['tidy_address']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb83GUZ_2GYp"
      },
      "source": [
        "def clean_csv_incident_characteristics(current_csv_incident_characteristics):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_incident_characteristics['incident_characteristics']=current_csv_incident_characteristics['incident_characteristics'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_incident_characteristics['tidy_incident_characteristics'] = np.vectorize(remove_pattern)(current_csv_incident_characteristics['incident_characteristics'], \"@[\\w]*\")\n",
        "  \n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_incident_characteristics['tidy_incident_characteristics'] = current_csv_incident_characteristics['tidy_incident_characteristics'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_incident_characteristics = current_csv_incident_characteristics['tidy_incident_characteristics'].apply(lambda x: x.split())\n",
        "  tokenized_incident_characteristics.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_incident_characteristics = tokenized_incident_characteristics.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_incident_characteristics.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_incident_characteristics)):\n",
        "      tokenized_incident_characteristics[i] = ' '.join(tokenized_incident_characteristics[i])\n",
        "\n",
        "  current_csv_incident_characteristics['tidy_incident_characteristics'] = tokenized_incident_characteristics\n",
        "  current_csv_incident_characteristics['tidy_incident_characteristics'] = current_csv_incident_characteristics['tidy_incident_characteristics']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiQNQbbH2HxR"
      },
      "source": [
        "def clean_csv_notes(current_csv_notes):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_notes['notes']=current_csv_notes['notes'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_notes['tidy_notes'] = np.vectorize(remove_pattern)(current_csv_notes['notes'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_notes['tidy_notes'] = current_csv_notes['tidy_notes'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_notes = current_csv_notes['tidy_notes'].apply(lambda x: x.split())\n",
        "  tokenized_notes.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_notes = tokenized_notes.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_notes.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_notes)):\n",
        "      tokenized_notes[i] = ' '.join(tokenized_notes[i])\n",
        "\n",
        "  current_csv_notes['tidy_notes'] = tokenized_notes\n",
        "  current_csv_notes['tidy_notes'] = current_csv_notes['tidy_notes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S65VGt0G2Ji9"
      },
      "source": [
        "def clean_csv_participant_age_group(current_csv_participant_age_group):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_age_group['participant_age_group']=current_csv_participant_age_group['participant_age_group'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_age_group['tidy_participant_age_group'] = np.vectorize(remove_pattern)(current_csv_participant_age_group['participant_age_group'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_age_group['tidy_participant_age_group'] = current_csv_participant_age_group['tidy_participant_age_group'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_age_group = current_csv_participant_age_group['tidy_participant_age_group'].apply(lambda x: x.split())\n",
        "  tokenized_participant_age_group.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_age_group = tokenized_participant_age_group.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_age_group.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_age_group)):\n",
        "      tokenized_participant_age_group[i] = ' '.join(tokenized_participant_age_group[i])\n",
        "\n",
        "  current_csv_participant_age_group['tidy_participant_age_group'] = tokenized_participant_age_group\n",
        "  current_csv_participant_age_group['tidy_participant_age_group'] = current_csv_participant_age_group['tidy_participant_age_group']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef65m_Wc2K5Z"
      },
      "source": [
        "def clean_csv_participant_gender(current_csv_participant_gender):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_gender['participant_gender']=current_csv_participant_gender['participant_gender'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_gender['tidy_participant_gender'] = np.vectorize(remove_pattern)(current_csv_participant_gender['participant_gender'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_gender['tidy_participant_gender'] = current_csv_participant_gender['tidy_participant_gender'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_gender = current_csv_participant_gender['tidy_participant_gender'].apply(lambda x: x.split())\n",
        "  tokenized_participant_gender.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_gender = tokenized_participant_gender.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_gender.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_gender)):\n",
        "      tokenized_participant_gender[i] = ' '.join(tokenized_participant_gender[i])\n",
        "\n",
        "  current_csv_participant_gender['tidy_participant_gender'] = tokenized_participant_gender\n",
        "  current_csv_participant_gender['tidy_participant_gender'] = current_csv_participant_gender['tidy_participant_gender']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxW4whSS2M66"
      },
      "source": [
        "def clean_csv_participant_name(current_csv_participant_name):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_name['participant_name']=current_csv_participant_name['participant_name'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_name['tidy_participant_name'] = np.vectorize(remove_pattern)(current_csv_participant_name['participant_name'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_name['tidy_participant_name'] = current_csv_participant_name['tidy_participant_name'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_name = current_csv_participant_name['tidy_participant_name'].apply(lambda x: x.split())\n",
        "  tokenized_participant_name.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_name = tokenized_participant_name.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_name.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_name)):\n",
        "      tokenized_participant_name[i] = ' '.join(tokenized_participant_name[i])\n",
        "\n",
        "  current_csv_participant_name['tidy_participant_name'] = tokenized_participant_name\n",
        "  current_csv_participant_name['tidy_participant_name'] = current_csv_participant_name['tidy_participant_name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ5G6e-82eqc"
      },
      "source": [
        "def clean_csv_participant_relationship(current_csv_participant_relationship):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_relationship['participant_relationship']=current_csv_participant_relationship['participant_relationship'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_relationship['tidy_participant_relationship'] = np.vectorize(remove_pattern)(current_csv_participant_relationship['participant_relationship'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_relationship['tidy_participant_relationship'] = current_csv_participant_relationship['tidy_participant_relationship'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_relationship = current_csv_participant_relationship['tidy_participant_relationship'].apply(lambda x: x.split())\n",
        "  tokenized_participant_relationship.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_relationship = tokenized_participant_relationship.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_relationship.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_relationship)):\n",
        "      tokenized_participant_relationship[i] = ' '.join(tokenized_participant_relationship[i])\n",
        "\n",
        "  current_csv_participant_relationship['tidy_participant_relationship'] = tokenized_participant_relationship\n",
        "  current_csv_participant_relationship['tidy_participant_relationship'] = current_csv_participant_relationship['tidy_participant_relationship']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hyT2dJR2hF7"
      },
      "source": [
        "def clean_csv_participant_status(current_csv_participant_status):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_status['participant_status']=current_csv_participant_status['participant_status'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_status['tidy_participant_status'] = np.vectorize(remove_pattern)(current_csv_participant_status['participant_status'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_status['tidy_participant_status'] = current_csv_participant_status['tidy_participant_status'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_status = current_csv_participant_status['tidy_participant_status'].apply(lambda x: x.split())\n",
        "  tokenized_participant_status.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_status = tokenized_participant_status.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_status.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_status)):\n",
        "      tokenized_participant_status[i] = ' '.join(tokenized_participant_status[i])\n",
        "\n",
        "  current_csv_participant_status['tidy_participant_status'] = tokenized_participant_status\n",
        "  current_csv_participant_status['tidy_participant_status'] = current_csv_participant_status['tidy_participant_status']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO8WvJCU2kti"
      },
      "source": [
        "def clean_csv_participant_type(current_csv_participant_type):\n",
        " \n",
        "  #Make Empty Spaces Strings\n",
        "  current_csv_participant_type['participant_type']=current_csv_participant_type['participant_type'].apply(str)\n",
        "\n",
        "  # add it in\n",
        "  current_csv_participant_type['tidy_participant_type'] = np.vectorize(remove_pattern)(current_csv_participant_type['participant_type'], \"@[\\w]*\")\n",
        "\n",
        "  # remove special characters, numbers, punctuations\n",
        "  current_csv_participant_type['tidy_participant_type'] = current_csv_participant_type['tidy_participant_type'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  \n",
        "  #tokenize the words\n",
        "  tokenized_participant_type = current_csv_participant_type['tidy_participant_type'].apply(lambda x: x.split())\n",
        "  tokenized_participant_type.head()\n",
        " \n",
        "  #Stem the words\n",
        "  stemmer = PorterStemmer()\n",
        "  tokenized_participant_type = tokenized_participant_type.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "  tokenized_participant_type.head()\n",
        " \n",
        "  #Add them back to csv\n",
        "  for i in range(len(tokenized_participant_type)):\n",
        "      tokenized_participant_type[i] = ' '.join(tokenized_participant_type[i])\n",
        "\n",
        "  current_csv_participant_type['tidy_participant_type'] = tokenized_participant_type\n",
        "  current_csv_participant_type['tidy_participant_type'] = current_csv_participant_type['tidy_participant_type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-mqKgsXPtWj"
      },
      "source": [
        "**VVV WHERE YOU WILL SAVE THE FILES VVV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_l9eVuGhCVZ"
      },
      "source": [
        "fileOpen = \"/content/drive/MyDrive/Stage2Clean\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98tbwm5ag2rA"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(\"/content/drive/MyDrive/Stage2Clean\") if isfile(join(\"/content/drive/MyDrive/Stage2Clean\", f))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk7ucCVNPjSX"
      },
      "source": [
        "**VVV ENTER FOLDER WITH DATA THAT NEEDS TO BE CLEANED HERE VVV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5arzvuW2oci"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(\"/content/Stage2\") if isfile(join(\"/content/Stage2\", f))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfIbzUYaPQmx"
      },
      "source": [
        "**VVV ENTER FOLDER WITH DATA THAT NEEDS TO BE CLEANED HERE VVV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tio4Fjol2qjx"
      },
      "source": [
        "fileOpen = \"/content/Stage2/\"\n",
        "for i in range(len(onlyfiles)):\n",
        "  temp = fileOpen + onlyfiles[i] \n",
        "  fileLoc = pd.read_csv(temp)\n",
        "  clean_csv_state(fileLoc)\n",
        "  clean_csv_cityorcounty(fileLoc)\n",
        "  clean_csv_address(fileLoc)\n",
        "  clean_csv_incident_characteristics(fileLoc)\n",
        "  clean_csv_notes(fileLoc)\n",
        "  clean_csv_participant_age_group(fileLoc)\n",
        "  clean_csv_participant_gender(fileLoc)\n",
        "  clean_csv_participant_name(fileLoc)\n",
        "  clean_csv_participant_relationship(fileLoc)\n",
        "  clean_csv_participant_status(fileLoc)\n",
        "  clean_csv_participant_type(fileLoc)\n",
        "  path = \"/content/Stage2Clean/\"\n",
        "  newTemp = path + onlyfiles[i]\n",
        "  fileLoc.to_csv(newTemp, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}